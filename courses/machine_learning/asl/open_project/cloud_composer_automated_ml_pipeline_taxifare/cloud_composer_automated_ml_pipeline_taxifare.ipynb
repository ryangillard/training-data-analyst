{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"cloud_composer_automated_ml_pipeline_taxifare.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"id":"KesQRvxkm1PH","colab_type":"code","colab":{}},"source":["import os\n","BUCKET = 'qwiklabs-gcp-8923d4964bfbd247-bucket' # REPLACE WITH A BUCKET NAME (PUT YOUR PROJECT ID AND WE CREATE THE BUCKET ITSELF NEXT)\n","PROJECT = 'qwiklabs-gcp-8923d4964bfbd247' # REPLACE WITH YOUR PROJECT ID\n","REGION = 'us-central1' # REPLACE WITH YOUR REGION e.g. us-central1\n","\n","# do not change these\n","os.environ['PROJECT'] = PROJECT\n","os.environ['BUCKET'] =  BUCKET\n","os.environ['REGION'] = REGION"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TLTq9A_dm1PL","colab_type":"code","outputId":"5619ad91-841f-41ee-dc52-fd248fa2ad1f","colab":{}},"source":["%%bash\n","gcloud config set project $PROJECT\n","gcloud config set compute/region $REGION"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Updated property [core/project].\n","Updated property [compute/region].\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"L4MZZN-Om1PQ","colab_type":"code","colab":{}},"source":["# %%bash\n","# OUTDIR=gs://${BUCKET}/trained_model\n","# JOBNAME=cloud_composer_automated_ml_pipeline_taxifare_$(date -u +%y%m%d_%H%M%S)\n","# echo $OUTDIR $REGION $JOBNAME\n","# # Clear the Cloud Storage Bucket used for the training job\n","# gsutil -m rm -rf $OUTDIR\n","# gcloud ml-engine jobs submit training $JOBNAME \\\n","#    --region=$REGION \\\n","#    --module-name=trainer.task \\\n","#    --package-path=${PWD}/cloud_composer_automated_ml_pipeline_taxifare/trainer \\\n","#    --job-dir=$OUTDIR \\\n","#    --staging-bucket=gs://$BUCKET \\\n","#    --scale-tier=BASIC \\\n","#    --runtime-version=1.8"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HxR6rmGBm1PT","colab_type":"code","colab":{}},"source":["%bash\n","cd cloud_composer_automated_ml_pipeline_taxifare_module\n","touch README.md\n","python setup.py sdist"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fjzhe-U2m1PV","colab_type":"code","colab":{}},"source":["!gsutil cp cloud_composer_automated_ml_pipeline_taxifare_module/dist/cloud_composer_automated_ml_pipeline_taxifare-0.1.tar.gz gs://qwiklabs-gcp-8923d4964bfbd247-bucket/code/"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O241YkN7m1PX","colab_type":"text"},"source":["***\n","# Part Two: Setup a scheduled workflow with Cloud Composer\n","In this section you will complete a partially written training.py DAG file and copy it to the DAGS folder in your Composer instance."]},{"cell_type":"markdown","metadata":{"id":"BczwG3zsm1PY","colab_type":"text"},"source":["## Copy your Airflow bucket name\n","1. Navigate to your Cloud Composer [instance](https://console.cloud.google.com/composer/environments?project=)<br/><br/>\n","2. Select __DAGs Folder__<br/><br/>\n","3. You will be taken to the Google Cloud Storage bucket that Cloud Composer has created automatically for your Airflow instance<br/><br/>\n","4. __Copy the bucket name__ into the variable below (example: us-central1-composer-08f6edeb-bucket)"]},{"cell_type":"code","metadata":{"id":"1r6tYEaDm1PZ","colab_type":"code","colab":{}},"source":["AIRFLOW_BUCKET = 'us-central1-cloud-composer-automated-ml-pipeline-taxifare-191f74a9-bucket' # REPLACE WITH AIRFLOW BUCKET NAME\n","os.environ['AIRFLOW_BUCKET'] = AIRFLOW_BUCKET"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7viNmxubm1Pb","colab_type":"text"},"source":["## Complete the training.py DAG file\n","Apache Airflow orchestrates tasks out to other services through a [DAG (Directed Acyclic Graph)](https://airflow.apache.org/concepts.html) file which specifies what services to call, what to do, and when to run these tasks. DAG files are written in python and are loaded automatically into Airflow once present in the Airflow/dags/ folder in your Cloud Composer bucket. \n","\n","Your task is to complete the partially written DAG file below which will enable the automatic retraining and redeployment of our WALS recommendation model. \n","\n","__Complete the #TODOs__ in the Airflow DAG file below and execute the code block to save the file"]},{"cell_type":"code","metadata":{"id":"OyHTWwY1m1Pc","colab_type":"code","outputId":"40a1668c-7e77-421a-fd96-d4d8d12b4e54","colab":{}},"source":["%%writefile airflow/dags/training.py\n","# Copyright 2018 Google Inc. All Rights Reserved.\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","# http://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","\n","\"\"\"DAG definition for anadarko composer toy model training.\"\"\"\n","\n","import airflow\n","from airflow import DAG\n","\n","# Reference for all available airflow operators: \n","# https://github.com/apache/incubator-airflow/tree/master/airflow/contrib/operators\n","from airflow.contrib.operators.bigquery_check_operator import BigQueryCheckOperator\n","from airflow.contrib.operators.bigquery_operator import BigQueryOperator\n","from airflow.contrib.operators.bigquery_to_gcs import BigQueryToCloudStorageOperator\n","from airflow.operators.bash_operator import BashOperator\n","from airflow.operators.python_operator import BranchPythonOperator\n","from airflow.operators.dummy_operator import DummyOperator\n","from airflow.hooks.base_hook import BaseHook\n","\n","from airflow.contrib.operators.mlengine_operator import MLEngineTrainingOperator, MLEngineModelOperator, MLEngineVersionOperator\n","from airflow.models import TaskInstance\n","\n","import datetime\n","import logging\n","\n","def _get_project_id():\n","    \"\"\"Get project ID from default GCP connection.\"\"\"\n","\n","    extras = BaseHook.get_connection(\"google_cloud_default\").extra_dejson\n","    key = \"extra__google_cloud_platform__project\"\n","    if key in extras:\n","        project_id = extras[key]\n","    else:\n","        raise (\"Must configure project_id in google_cloud_default \"\n","                     \"connection from Airflow Console\")\n","    return project_id\n","\n","PROJECT_ID = _get_project_id()\n","\n","# Data set constants, used in BigQuery tasks.    You can change these\n","# to conform to your data.\n","\n","# TODO: Specify your BigQuery dataset name and table name\n","DATASET = \"yellow\"\n","TABLE_NAME = \"trips\"\n","\n","# TODO: Confirm bucket name and region\n","# GCS bucket names and region, can also be changed.\n","BUCKET = \"gs://\" + PROJECT_ID + \"-bucket\"\n","REGION = \"us-central1\"\n","\n","# # The code package name comes from the model code in the wals_ml_engine\n","# # directory of the solution code base.\n","PACKAGE_URI = BUCKET + \"/code/cloud_composer_automated_ml_pipeline_taxifare-0.1.tar.gz\"\n","JOB_DIR = BUCKET + \"/jobs\"\n","\n","default_args = {\n","        \"owner\": \"airflow\",\n","        \"depends_on_past\": False,\n","        \"start_date\": airflow.utils.dates.days_ago(2),\n","        \"email\": [\"airflow@example.com\"],\n","        \"email_on_failure\": True,\n","        \"email_on_retry\": False,\n","        \"retries\": 5,\n","        \"retry_delay\": datetime.timedelta(minutes = 5)\n","}\n","\n","# Default schedule interval using cronjob syntax - can be customized here\n","# or in the Airflow console.\n","\n","# TODO: Specify a schedule interval in CRON syntax to run once a day at 2100 hours (9pm)\n","# Reference: https://airflow.apache.org/scheduler.html\n","schedule_interval = \"00 21 * * *\"\n","\n","# TODO: Title your DAG to be recommendations_training_v1\n","dag = DAG(\"cloud_composer_automated_ml_pipeline_taxifare\", \n","                    default_args = default_args,\n","                    schedule_interval = schedule_interval)\n","\n","dag.doc_md = __doc__\n","\n","\n","#\n","#\n","# Task Definition\n","#\n","#\n","\n","# BigQuery data query\n","bql=\"\"\"\n","#standardsql\n","SELECT\n","    (tolls_amount + fare_amount) AS fare_amount,\n","    EXTRACT(DAYOFWEEK FROM pickup_datetime) * 1.0 AS dayofweek,\n","    EXTRACT(HOUR FROM pickup_datetime) * 1.0 AS hourofday,\n","    pickup_longitude AS pickuplon,\n","    pickup_latitude AS pickuplat,\n","    dropoff_longitude AS dropofflon,\n","    dropoff_latitude AS dropofflat,\n","    passenger_count*1.0 AS passengers,\n","    CONCAT(CAST(pickup_datetime AS STRING), CAST(pickup_longitude AS STRING), CAST(pickup_latitude AS STRING), CAST(dropoff_latitude AS STRING), CAST(dropoff_longitude AS STRING)) AS key\n","FROM\n","    `{0}.{1}.{2}`\n","WHERE\n","    trip_distance > 0\n","    AND fare_amount >= 2.5\n","    AND pickup_longitude > -78\n","    AND pickup_longitude < -70\n","    AND dropoff_longitude > -78\n","    AND dropoff_longitude < -70\n","    AND pickup_latitude > 37\n","    AND pickup_latitude < 45\n","    AND dropoff_latitude > 37\n","    AND dropoff_latitude < 45\n","    AND passenger_count > 0\n","    AND RAND() < 0.0001\n","\"\"\"\n","\n","bql = bql.format(PROJECT_ID, DATASET, TABLE_NAME)\n","\n","bql_train = \"{0} AND MOD(ABS(FARM_FINGERPRINT(CAST(pickup_datetime AS STRING))), 5) < 4\".format(bql)\n","bql_eval = \"{0} AND MOD(ABS(FARM_FINGERPRINT(CAST(pickup_datetime AS STRING))), 5) = 4\".format(bql)\n","\n","# TODO: Complete the BigQueryOperator task to truncate the table if it already exists before writing\n","# Reference: https://airflow.apache.org/integration.html#bigqueryoperator\n","bq_train_data_op = BigQueryOperator(\n","    task_id = \"bq_train_data_task\",\n","    bql = bql_train,\n","    destination_dataset_table = \"%s.cloud_composer_automated_ml_pipeline_taxifare_train_data\" % DATASET,\n","    write_disposition = \"WRITE_TRUNCATE\", # specify to truncate on writes\n","    use_legacy_sql = False,\n","    dag = dag\n",")\n","\n","bq_eval_data_op = BigQueryOperator(\n","    task_id = \"bq_eval_data_task\",\n","    bql = bql_eval,\n","    destination_dataset_table = \"%s.cloud_composer_automated_ml_pipeline_taxifare_eval_data\" % DATASET,\n","    write_disposition = \"WRITE_TRUNCATE\", # specify to truncate on writes\n","    use_legacy_sql = False,\n","    dag = dag\n",")\n","\n","sql = \"\"\"\n","SELECT\n","    COUNT(*)\n","FROM\n","    [{0}:{1}.{2}]\n","\"\"\"\n","sql_check_train = sql.format(PROJECT_ID, DATASET, \"cloud_composer_automated_ml_pipeline_taxifare_train_data\")\n","sql_check_eval = sql.format(PROJECT_ID, DATASET, \"cloud_composer_automated_ml_pipeline_taxifare_eval_data\")\n","\n","# Check to make sure that the data tables won\"t be empty\n","bq_check_train_data_op = BigQueryCheckOperator(\n","    task_id = \"bq_check_train_data_task\",\n","    sql = sql_check_train,\n","    dag = dag\n",")\n","\n","bq_check_eval_data_op = BigQueryCheckOperator(\n","    task_id = \"bq_check_eval_data_task\",\n","    sql = sql_check_eval,\n","    dag = dag\n",")\n","\n","# BigQuery training data export to GCS\n","bash_remove_old_data_op = BashOperator(\n","    task_id = \"bash_remove_old_data_task\",\n","    bash_command = \"gsutil -m rm -rf {0}/data/*\".format(BUCKET),\n","    dag = dag\n",")\n","\n","# TODO: Fill in the missing operator name for task #2 which\n","# takes a BigQuery dataset and table as input and exports it to GCS as a CSV\n","train_files = BUCKET + \"/data/cloud_composer_automated_ml_pipeline_taxifare/train-*.csv\"\n","\n","bq_export_gcs_train_csv_op = BigQueryToCloudStorageOperator(\n","    task_id = \"bq_export_gcs_train_csv_task\",\n","    source_project_dataset_table = \"%s.cloud_composer_automated_ml_pipeline_taxifare_train_data\" % DATASET,\n","    destination_cloud_storage_uris = [train_files],\n","    export_format = \"CSV\",\n","    print_header = False,\n","    dag = dag\n",")\n","\n","eval_files = BUCKET + \"/data/cloud_composer_automated_ml_pipeline_taxifare/eval-*.csv\"\n","\n","bq_export_gcs_eval_csv_op = BigQueryToCloudStorageOperator(\n","    task_id = \"bq_export_gcs_eval_csv_task\",\n","    source_project_dataset_table = \"%s.cloud_composer_automated_ml_pipeline_taxifare_eval_data\" % DATASET,\n","    destination_cloud_storage_uris = [eval_files],\n","    export_format = \"CSV\",\n","    print_header = False,\n","    dag = dag\n",")\n","\n","\n","# # ML Engine training job\n","job_id = \"cloud_composer_automated_ml_pipeline_taxifare_{0}\".format(datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n","output_dir = BUCKET + \"/trained_model\"\n","job_dir = BUCKET + \"/jobs/\" + job_id\n","training_args = [\"--job-dir\", job_dir]\n","\n","# # TODO: Fill in the missing operator name for task #3 which will start a new training job to Cloud ML Engine\n","# # Reference: https://airflow.apache.org/integration.html#cloud-ml-engine\n","# # https://cloud.google.com/ml-engine/docs/tensorflow/machine-types\n","ml_engine_training_op = MLEngineTrainingOperator(\n","    task_id = \"ml_engine_training_task\",\n","    project_id = PROJECT_ID,\n","    job_id = job_id,\n","    package_uris = [PACKAGE_URI],\n","    training_python_module = \"trainer.task\",\n","    training_args = training_args,\n","    region = REGION,\n","    scale_tier = \"BASIC\",\n","    runtime_version = \"1.13\", \n","    python_version = \"3.5\",\n","    dag = dag\n",")\n","\n","MODEL_NAME = \"cloud_composer_automated_ml_pipeline_taxifare\"\n","MODEL_VERSION = \"v1\"\n","MODEL_LOCATION = BUCKET + \"/saved_model\"\n","\n","bash_remove_old_saved_model_op = BashOperator(\n","    task_id = \"bash_remove_old_saved_model_task\",\n","    bash_command = \"gsutil -m rm -rf {0}/*\".format(MODEL_LOCATION),\n","    dag = dag\n",")\n","\n","bash_copy_new_saved_model_op = BashOperator(\n","    task_id = \"bash_copy_new_saved_model_task\",\n","    bash_command = \"gsutil -m rsync -d -r `gsutil ls {0}/export/exporter/ | tail -1` {1}\".format(output_dir, MODEL_LOCATION),\n","    dag = dag\n",")\n","\n","# Create model on ML-Engine\n","bash_ml_engine_models_list_op = BashOperator(\n","    task_id = \"bash_ml_engine_models_list_task\",\n","    xcom_push = True,\n","    bash_command = \"gcloud ml-engine models list --filter=\"name:{0}\"\".format(MODEL_NAME),\n","    dag = dag\n",")\n","\n","def check_if_model_already_exists(**kwargs):\n","    ml_engine_models_list = kwargs[\"ti\"].xcom_pull(task_ids = \"bash_ml_engine_models_list_task\")\n","    logging.info(\"check_if_model_already_exists: ml_engine_models_list = \\n{}\".format(ml_engine_models_list))\n","    if len(ml_engine_models_list) == 0 or ml_engine_models_list == \"Listed 0 items.\":\n","        return \"ml_engine_create_model_task\"\n","    return \"dont_create_model_dummy_branch_task\"\n","\n","check_if_model_already_exists_op = BranchPythonOperator(\n","    task_id = \"check_if_model_already_exists_task\", \n","    python_callable = check_if_model_already_exists,\n","    provide_context = True,\n","    dag = dag\n",")\n","\n","ml_engine_create_model_op = MLEngineModelOperator(\n","    task_id = \"ml_engine_create_model_task\",\n","    project_id = PROJECT_ID, \n","    model = {\"name\": MODEL_NAME}, \n","    operation = \"create\",\n","    dag = dag\n",")\n","\n","create_model_dummy_op = DummyOperator(\n","    task_id = \"create_model_dummy_task\",\n","    trigger_rule = \"all_done\",\n","    dag = dag\n",")\n","\n","dont_create_model_dummy_branch_op = DummyOperator(\n","    task_id = \"dont_create_model_dummy_branch_task\",\n","    dag = dag\n",")\n","\n","dont_create_model_dummy_op = DummyOperator(\n","    task_id = \"dont_create_model_dummy_task\",\n","    trigger_rule = \"all_done\",\n","    dag = dag\n",")\n","\n","# Create version of model on ML-Engine\n","bash_ml_engine_versions_list_op = BashOperator(\n","    task_id = \"bash_ml_engine_versions_list_task\",\n","    xcom_push = True,\n","    bash_command = \"gcloud ml-engine versions list --model {0} --filter=\"name:{1}\"\".format(MODEL_NAME, MODEL_VERSION),\n","    dag = dag\n",")\n","\n","def check_if_model_version_already_exists(**kwargs):\n","    ml_engine_versions_list = kwargs[\"ti\"].xcom_pull(task_ids = \"bash_ml_engine_versions_list_task\")\n","    logging.info(\"check_if_model_version_already_exists: ml_engine_versions_list = \\n{}\".format(ml_engine_versions_list))\n","    if len(ml_engine_versions_list) == 0 or ml_engine_versions_list == \"Listed 0 items.\":\n","        return \"ml_engine_create_version_task\"\n","    return \"ml_engine_create_other_version_task\"\n","\n","check_if_model_version_already_exists_op = BranchPythonOperator(\n","    task_id = \"check_if_model_version_already_exists_task\", \n","    python_callable = check_if_model_version_already_exists,\n","    provide_context = True,\n","    dag = dag\n",")\n","\n","OTHER_VERSION_NAME = \"v_{0}\".format(datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")[0:12])\n","\n","ml_engine_create_version_op = MLEngineVersionOperator(\n","    task_id = \"ml_engine_create_version_task\",\n","    project_id = PROJECT_ID, \n","    model_name = MODEL_NAME, \n","    version_name = MODEL_VERSION, \n","    version = {\"name\": MODEL_VERSION, \"deploymentUri\": MODEL_LOCATION}, \n","    operation = \"create\",\n","    dag = dag\n",")\n","\n","ml_engine_create_other_version_op = MLEngineVersionOperator(\n","    task_id = \"ml_engine_create_other_version_task\",\n","    project_id = PROJECT_ID, \n","    model_name = MODEL_NAME, \n","    version_name = OTHER_VERSION_NAME, \n","    version = {\"name\": OTHER_VERSION_NAME, \"deploymentUri\": MODEL_LOCATION}, \n","    operation = \"create\",\n","    dag = dag\n",")\n","\n","ml_engine_set_default_version_op = MLEngineVersionOperator(\n","    task_id = \"ml_engine_set_default_version_task\",\n","    project_id = PROJECT_ID, \n","    model_name = MODEL_NAME, \n","    version_name = MODEL_VERSION, \n","    version = {\"name\": MODEL_VERSION}, \n","    operation = \"set_default\",\n","    dag = dag\n",")\n","\n","ml_engine_set_default_other_version_op = MLEngineVersionOperator(\n","    task_id = \"ml_engine_set_default_other_version_task\",\n","    project_id = PROJECT_ID, \n","    model_name = MODEL_NAME, \n","    version_name = OTHER_VERSION_NAME, \n","    version = {\"name\": OTHER_VERSION_NAME}, \n","    operation = \"set_default\",\n","    dag = dag\n",")\n","\n","# TODO: Be sure to set_upstream dependencies for all tasks\n","bq_check_train_data_op.set_upstream(bq_train_data_op)\n","bq_check_eval_data_op.set_upstream(bq_eval_data_op)\n","\n","bash_remove_old_data_op.set_upstream([bq_check_train_data_op, bq_check_eval_data_op])\n","\n","bq_export_gcs_train_csv_op.set_upstream([bq_train_data_op, bash_remove_old_data_op])\n","bq_export_gcs_eval_csv_op.set_upstream([bq_eval_data_op, bash_remove_old_data_op])\n","\n","ml_engine_training_op.set_upstream([bq_export_gcs_train_csv_op, bq_export_gcs_eval_csv_op])\n","\n","bash_remove_old_saved_model_op.set_upstream(ml_engine_training_op)\n","bash_copy_new_saved_model_op.set_upstream(bash_remove_old_saved_model_op)\n","\n","bash_ml_engine_models_list_op.set_upstream(ml_engine_training_op)\n","check_if_model_already_exists_op.set_upstream(bash_ml_engine_models_list_op)\n","\n","ml_engine_create_model_op.set_upstream(check_if_model_already_exists_op)\n","create_model_dummy_op.set_upstream(ml_engine_create_model_op)\n","dont_create_model_dummy_branch_op.set_upstream(check_if_model_already_exists_op)\n","dont_create_model_dummy_op.set_upstream(dont_create_model_dummy_branch_op)\n","\n","bash_ml_engine_versions_list_op.set_upstream([dont_create_model_dummy_op, create_model_dummy_op])\n","check_if_model_version_already_exists_op.set_upstream(bash_ml_engine_versions_list_op)\n","\n","ml_engine_create_version_op.set_upstream([bash_copy_new_saved_model_op, check_if_model_version_already_exists_op])\n","ml_engine_create_other_version_op.set_upstream([bash_copy_new_saved_model_op, check_if_model_version_already_exists_op])\n","\n","ml_engine_set_default_version_op.set_upstream(ml_engine_create_version_op)\n","ml_engine_set_default_other_version_op.set_upstream(ml_engine_create_other_version_op)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Overwriting airflow/dags/training.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"plQuoTu1m1Pf","colab_type":"code","outputId":"f4f2e393-52f6-4b03-a24c-c5d68514b80a","colab":{}},"source":["import datetime\n","string = \"v_{0}\".format(datetime.datetime.now().strftime('%Y%m%d%H%M%S')[0:12])\n","print(string)\n","print(len(string))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["v_201901160411\n","14\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LCdHW7wrm1Pj","colab_type":"code","colab":{}},"source":["# !gsutil -m cp -r gs://qwiklabs-gcp-8923d4964bfbd247-bucket/models-v1 ."],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2-YF4aOUm1Pl","colab_type":"text"},"source":["### Copy local Airflow DAG file and plugins into the DAGs folder"]},{"cell_type":"code","metadata":{"id":"KAd9I3icm1Pm","colab_type":"code","colab":{}},"source":["%bash\n","gsutil cp airflow/dags/training.py gs://${AIRFLOW_BUCKET}/dags # overwrite if it exists"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0ixqSbrmm1Po","colab_type":"text"},"source":["2. Navigate to your Cloud Composer [instance](https://console.cloud.google.com/composer/environments?project=)<br/><br/>\n","\n","3. Trigger a __manual run__ of your DAG for testing<br/><br/>\n","\n","3. Ensure your DAG runs successfully (all nodes outlined in dark green and 'success' tag shows)\n","\n","![Successful Airflow DAG run](./img/airflow_successful_run.jpg \"Successful Airflow DAG run\")\n"]},{"cell_type":"code","metadata":{"id":"vFw_5flVm1Pp","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}